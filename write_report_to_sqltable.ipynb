{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e08295-0c46-43f3-8c16-f65ed3e37a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-183043315165777>, line 7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m cols \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisruption_reason\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mline\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m      6\u001B[0m tube_reports \u001B[38;5;241m=\u001B[39m get_tube_disruption_reports()\n",
       "\u001B[0;32m----> 7\u001B[0m tube_reports_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(tube_reports,cols)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:400\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema)\u001B[0m\n",
       "\u001B[1;32m    398\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(LocalRelation(table\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, schema\u001B[38;5;241m=\u001B[39m_schema\u001B[38;5;241m.\u001B[39mjson()), \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m    399\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 400\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m    401\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    402\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{},\n",
       "\u001B[1;32m    403\u001B[0m         )\n",
       "\u001B[1;32m    405\u001B[0m _table: Optional[pa\u001B[38;5;241m.\u001B[39mTable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m    408\u001B[0m     \u001B[38;5;66;03m# Logic was borrowed from `_create_from_pandas_with_arrow` in\u001B[39;00m\n",
       "\u001B[1;32m    409\u001B[0m     \u001B[38;5;66;03m# `pyspark.sql.pandas.conversion.py`. Should ideally deduplicate the logics.\u001B[39;00m\n",
       "\u001B[1;32m    410\u001B[0m \n",
       "\u001B[1;32m    411\u001B[0m     \u001B[38;5;66;03m# If no schema supplied by user then get the names of columns only\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mPySparkValueError\u001B[0m: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkValueError",
        "evalue": "[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset."
       },
       "metadata": {
        "errorSummary": "[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "CANNOT_INFER_EMPTY_SCHEMA",
        "sqlState": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkValueError\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-183043315165777>, line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m cols \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisruption_reason\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mline\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      6\u001B[0m tube_reports \u001B[38;5;241m=\u001B[39m get_tube_disruption_reports()\n\u001B[0;32m----> 7\u001B[0m tube_reports_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(tube_reports,cols)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:400\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    398\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(LocalRelation(table\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, schema\u001B[38;5;241m=\u001B[39m_schema\u001B[38;5;241m.\u001B[39mjson()), \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    399\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 400\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m    401\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    402\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    403\u001B[0m         )\n\u001B[1;32m    405\u001B[0m _table: Optional[pa\u001B[38;5;241m.\u001B[39mTable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m    408\u001B[0m     \u001B[38;5;66;03m# Logic was borrowed from `_create_from_pandas_with_arrow` in\u001B[39;00m\n\u001B[1;32m    409\u001B[0m     \u001B[38;5;66;03m# `pyspark.sql.pandas.conversion.py`. Should ideally deduplicate the logics.\u001B[39;00m\n\u001B[1;32m    410\u001B[0m \n\u001B[1;32m    411\u001B[0m     \u001B[38;5;66;03m# If no schema supplied by user then get the names of columns only\u001B[39;00m\n",
        "\u001B[0;31mPySparkValueError\u001B[0m: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from get_latest_tube_distruption import get_tube_disruption_reports\n",
    "\n",
    "\n",
    "cols = [\"current_timestamp\",\"disruption_reason\",\"line\",\"status\"]\n",
    "tube_reports = get_tube_disruption_reports()\n",
    "tube_reports_df = spark.createDataFrame(tube_reports,cols)\n",
    "\n",
    "tube_reports_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"latest_tube_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b17d0ce-17dc-4c12-9e97-818e3c587195",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_timestamp</th><th>line</th><th>status</th><th>disruption_reason</th></tr></thead><tbody><tr><td>2024-03-19T12:07:25Z</td><td>Hammersmith & City</td><td>Minor Delays</td><td>Hammersmith and City Line: Minor delays due to train cancellations. </td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-03-19T12:07:25Z",
         "Hammersmith & City",
         "Minor Delays",
         "Hammersmith and City Line: Minor delays due to train cancellations. "
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "current_timestamp",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "line",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "disruption_reason",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 31
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "current_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "line",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "disruption_reason",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from latest_tube_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffeb5d52-3d9b-4c5d-9f99-c81e771df8a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pyspark.sql.connect.dataframe.DataFrame"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58d18852-f43d-486c-879b-cdbe053c44a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3815452737040866,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "write_report_to_sqltable",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
